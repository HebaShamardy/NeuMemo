import { initializeApp } from "firebase/app";
import { getAI, getGenerativeModel, GoogleAIBackend, InferenceMode, Schema } from "firebase/ai";
import { get_encoding } from "tiktoken";

// TODO(developer) Replace the following with your app's Firebase configuration
// See: https://firebase.google.com/docs/web/learn-more#config-object
// Firebase config moved to environment variables.
// For Vite, expose client env vars with the VITE_ prefix and access them
// via `import.meta.env.VITE_*`. See `.env.example` for variable names.
// Destructure the Vite env vars and use them directly in the config object.
const {
  VITE_FIREBASE_API_KEY: apiKey,
  VITE_FIREBASE_AUTH_DOMAIN: authDomain,
  VITE_FIREBASE_PROJECT_ID: projectId,
  VITE_FIREBASE_STORAGE_BUCKET: storageBucket,
  VITE_FIREBASE_MESSAGING_SENDER_ID: messagingSenderId,
  VITE_FIREBASE_APP_ID: appId,
  VITE_FIREBASE_MEASUREMENT_ID: measurementId
} = import.meta.env;

const firebaseConfig = { apiKey, authDomain, projectId, storageBucket, messagingSenderId, appId, measurementId };


// Initialize FirebaseApp
const firebaseApp = initializeApp(firebaseConfig);
// Initialize the Gemini Developer API backend service
const ai = getAI(firebaseApp, { backend: new GoogleAIBackend() });

// The model will return an ARRAY of tab objects. Each item follows the
// structure defined below.
const responseSchema = Schema.array({
        items: Schema.object({
                properties: {
                        tab_id: Schema.string(),
                        session_name: Schema.string(),
                        summarized_content: Schema.string(),
                }
        })
});
const promptTemplate = `You're an AI session manager for browser tabs.

You will receive multiple website tabs as input.  
Each tab is separated by the token <NEMO_tab> and includes:
- url
- title
- content (the webpage text or body)

Your goal:
1. Analyze all tabs together to detect related topics or user intents.
2. **Group related tabs into shared sessions** ‚Äî tabs discussing the same subject, product, technology, or goal must share the same session name.
   - Merge closely related topics under one common \`session_name\`.
   - Prefer reusing concise session names instead of creating new ones.
   - Example: Tabs about Gemini API, Prompt API, and Firebase AI Logic can all share a session like ‚ÄúGemini and Chrome AI Development.‚Äù
3. Assign each tab to the most relevant \`session_name\`.
4. Generate a **detailed factual summary** that captures important concepts, sections, or highlights for future search use.

### Output format (strict JSON array):
[
  {
    "tab_id": "<url>",
    "session_name": "<shared_topic_or_goal>",
    "summarized_content": "<informative summary capturing key ideas and sections>"
  },
  ...
]

### Output rules:
- Output only valid JSON.
- Tabs covering the same topic or workflow **must share the same session_name**.
- Summaries should capture meaningful sections and important context (3‚Äì6 sentences).
- Be factual, structured, and concise.
`;

const getModelConfig = () => ({
    mode: InferenceMode.PREFER_ON_DEVICE,
    model: "gemini-2.5-pro",
    generationConfig: {
        responseMimeType: "application/json",
        responseSchema: responseSchema,
        // Using a lower temperature for more predictable, structured JSON output.
        temperature: 1.0,
        stopSequences: ["]```"],
        topP: 1.0,
        // Stop generation when the closing bracket of the array is found.
        candidateCount: 1,
    },
});

// Create a `GenerativeModel` instance (lazy-created/re-creatable)
let model = getGenerativeModel(ai, getModelConfig());

function recreateModel() {
    // Recreate the generative model instance. Useful if the previous
    // execution session was destroyed or otherwise became invalid.
    model = getGenerativeModel(ai, getModelConfig());
}

// Imports + initialization of FirebaseApp and backend service + creation of model instance


// ... existing code ...

function normalizeContent(content) {
    if (!content) {
        return '';
    }
    // Trim leading/trailing whitespace.
    let cleanedContent = content.trim();

    // Replace multiple newlines (3 or more) with just two.
    cleanedContent = cleanedContent.replace(/(\r\n|\r|\n){3,}/g, '\n\n');

    return cleanedContent;
}

async function summarizeTabs(tabs, maxTokens = 1000000) {
    if (!Array.isArray(tabs)) {
        throw new Error('summarizeTabs expects an array of tabs');
    }

    const encoding = get_encoding("cl100k_base");

    let tabsInput = '';
    let tabsInPrompt = 0;
    let totalTokens = 0;

    for (const t of tabs) {
        const tabHeader = `\n<NEMO_tab>\nTitle: ${t.title}\nURL: ${t.url}\nContent: `;
        const headerTokens = encoding.encode(tabHeader).length;
        
        if (totalTokens + headerTokens > maxTokens) {
            console.warn('Skipping a tab as the prompt is already too large for more tabs.');
            break; // No more space for even a header
        }

        let availableTokens = maxTokens - totalTokens - headerTokens;
        const normalizedContent = normalizeContent(t.content);
        let contentTokens = encoding.encode(normalizedContent);
        
        let truncatedContent;
        if (contentTokens.length > availableTokens) {
            truncatedContent = new TextDecoder().decode(encoding.decode(contentTokens.slice(0, availableTokens)));
            console.warn(`Content for tab ${t.url} was truncated to fit within the token limit.`);
        } else {
            truncatedContent = normalizedContent;
        }

        const finalTabString = `${tabHeader}${truncatedContent}`;
        tabsInput += finalTabString;
        totalTokens += encoding.encode(finalTabString).length;
        tabsInPrompt++;
    }

    console.log(`üìä Prompt includes ${tabsInPrompt} of ${tabs.length} tabs provided.`);
    console.log(`üìä Final prompt token count: ${totalTokens}`);

    const finalPrompt = `${promptTemplate}\n${tabsInput}\n\nPlease output ONLY the JSON array of tab objects (no surrounding text).`;
    console.log("üìù Full AI Request Prompt:", finalPrompt);

    const request = {
        contents: [
            { role: 'user', parts: [{ text: finalPrompt }] }
        ]
    };

    try {
        const result = await model.generateContent(request);
        let responseText = result.response.text();
        console.log("ü§ñ Raw AI Response:", responseText);
        let cleaned = responseText.trim().replace(/^```json/, '').replace(/```$/,'').trim();

        try {
            // First attempt to parse the cleaned response
            const parsed = JSON.parse(cleaned);
            if (!Array.isArray(parsed)) {
                throw new Error('AI response was not an array');
            }
            return parsed;

        } catch (jsonError) {
            console.warn("‚ö†Ô∏è JSON parsing failed. Attempting to have the model fix it.", jsonError.message);

            // Build a multi-turn fix-up request preserving context and reinforcing correctness.
            const fixupInstruction = `Regenerate the JSON to be complete for all TABS_COUNT tabs and include the actual data extracted from each tab.
Do not output placeholder null values unless the information truly does not exist.
Return only a valid JSON array with exactly TABS_COUNT objects that match the schema and rules described above.`.replace(/TABS_COUNT/g, String(tabsInPrompt));

            const fixupRequest = {
                contents: [
                    // Re-send the instructions at the start of the conversation
                    { role: 'user', parts: [{ text: promptTemplate }] },
                    // Re-send the original tabs input so the model has full context
                    { role: 'user', parts: [{ text: `Original tabs input (TABS_COUNT=${tabsInPrompt}):\n${tabsInput}` }] },
                    // Provide the model's previous (broken) response
                    { role: 'model', parts: [{ text: cleaned }] },
                    // Final instruction to regenerate fully populated, valid JSON
                    { role: 'user', parts: [{ text: fixupInstruction }] }
                ]
            };

            console.log("üîß Sending fix-up request to the model...");
            const fixupResult = await model.generateContent(fixupRequest);
            const fixedResponseText = fixupResult.response.text();
            console.log("ü§ñ Corrected AI Response:", fixedResponseText);
            
            const fixedCleaned = fixedResponseText.trim().replace(/^```json/, '').replace(/```$/,'').trim();
            
            try {
                const parsed = JSON.parse(fixedCleaned);
                if (!Array.isArray(parsed)) {
                    throw new Error('Corrected AI response was not an array');
                }
                return parsed;
            } catch (finalJsonError) {
                console.error("‚ùå JSON fix-up also failed. Returning empty array.", finalJsonError.message);
                return []; // Return empty array if the fix-up also fails
            }
        }
    } catch (error) {
        console.error("‚ùå Error during AI content generation or parsing. The model's response may have been invalid or truncated.", error);
        return []; // Return empty array on catastrophic failure
    }
}

// NOTE: do not auto-run the sample on module load. Extensions have strict
// CSP and running the model on load can cause unexpected API calls and
// errors (for example: missing output language). Call `run()` from the UI
// or expose it on window for manual invocation during development.
// Example (from viewer.js):
// import { run } from './firebase_ai.js';
// document.getElementById('generate').addEventListener('click', run);

export { summarizeTabs };
export { recreateModel };
